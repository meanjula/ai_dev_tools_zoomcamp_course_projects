# AI_Code_explainer_with_Ollama (Server)

 A full-stack app that uses ollama's llama3 model to explain code in plain language.

It Features:
- ğŸ§  local AI interface using ollama.
- âš™ï¸ Express.js to stream responses
- âš¡ React for UI

## Prerequisite
- **Node.js**,
-  **VS Code**,
- **Ollama** install locally
  â†’ [Download ollama](https://ollama.com/download)

## ğŸ—ï¸ Project Structure 
```bash

OLLAMA_APP/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ node_modules
â”‚   â”œâ”€â”€ package.json 
â”‚   â”œâ”€â”€ package-lock.json 
â”‚   â””â”€â”€ server.js                 
â”œâ”€â”€ frontend/      # React frontend app
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```
<img src="frontend/public/code-explanation.png" width="45%" style="margin: auto;" alt="app UI screenshot after response" />

## âš™ï¸ Server Setup Instructions For OllAMA_APP 

### Prerequisites

Make sure you have installed:

- Node.js **v18+**

#### Setup Ollama

```bash
ollama serve

ollama pull llama3

ollama list 

```
#### Express server setup
In the project root, inititae node project and install all dependencies
```bash

- npm init 
- npm install express node-fetch cors helmet
- npm run dev

```
### server.js

- create express server using express
- create security middleware using helmet and cors.
- create endpoint for calling api to explain code .POST method is used 
chunk is a Buffer or Uint8Array
convert raw binary chunk to string and split into array of lines using newline as separator and filter out empty lines, as in js Boolean("") is false
                   
.toString()
'{"response":"Hello"}\n{"response":"World"}\n'

.split("\n")
[
  '{"response":"Hello world"}',
  '{"response":"!"}',
  ''
]

.filter(Boolean)[
  '{"response":"Hello world"}',
  '{"response":"!"}'
]





ğŸ§  How It Works

Code explainer backend uses **Express.js** and **Ollama (Laama3)** to provide real time, AI-powered explanation for any code snippet.

---
### Client Sends Request  
THe frontend (React app) sends a 'POST' request wirh a jso body like:

```json
{
  "code":"function sum(a,b){ return a+b}",
  "language":"javascript"
}
```

### AI Prompt in server

The Express server validates the input, then creates a chat-style prompt for Ollama:
```bash
const message = [
  ```javascript
  {
    role: "user",
    content: `You are a senior ${language} developer and mentor.
    Explain the following ${language} code to a junior developer.
    Be accurate and concise, use Markdown formatting when useful.
    
    Code:
    \`\`\`${language}
    ${code}
    \`\`\``
    }
  ```
];
```
This prompt gives the LLM context and structure for producing a technical yet beginner-friendly explanation.

### Request Sent to Ollama
The backend sends this prompt to a locally running Ollama model via:
`POST http://localhost:11434/api/chat`
with streaming enabled:
```bash
{
  model: "llama3",
  stream: true,
  temperature: 0.3,
  max_tokens: 800
}
```

### Streaming NDJSON Response
Ollama responds token-by-token in NDJSON format (newline-delimited JSON).
The Express server forwards each chunks directly to the frontend as a live stream.

```bash
res.write(JSON.stringify({ type: "token", content: data.response }) + "\n");
```
### Frontend Renders in Real Time

The React app reads the streamed tokens and updates the UI instantly, showing the explanation as itâ€™s generated.
This gives a streaming experience, all powered locally via Ollama.

## Summary Flow

```scss
[ User ]
   â”‚
   â–¼
[ React Frontend ]
   â”‚  (POST /api/explain-code)
   â–¼
[ Express Server ]
   â”‚  (Forwards request to Ollama)
   â–¼
[ Ollama LLM ]
   â”‚  (Streams NDJSON tokens)
   â–¼
[ Express Streams Back to Client ]
   â”‚
   â–¼
[ React UI updates in real time ğŸ’¬ ]

```
## âš™ï¸ Why NDJSON Streaming?

Using NDJSON (Newline-Delimited JSON) allows:

Incremental reading (no need to wait for full response)

Real-time updates with minimal latency

Compatibility with both Node.js streams and browser ReadableStream